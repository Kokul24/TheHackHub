"""
Sakhi-Score: FastAPI Backend
Credit Scoring API with SHAP Explainability for Self-Help Groups
"""

import os
import pickle
import base64
from io import BytesIO
from datetime import datetime
from typing import Optional

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend for server
import matplotlib.pyplot as plt
import shap
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from langdetect import detect as lang_detect
from word2number import w2n
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure
from gtts import gTTS
import tempfile
import time
import unicodedata
from pathlib import Path

# Prefer faster-whisper if available for lower latency
try:
    from faster_whisper import WhisperModel as FasterWhisperModel
    _HAS_FASTER_WHISPER = True
except Exception:
    FasterWhisperModel = None
    _HAS_FASTER_WHISPER = False

try:
    import whisper
    _HAS_OPENAI_WHISPER = True
except Exception:
    whisper = None
    _HAS_OPENAI_WHISPER = False

# ============================================================
# FastAPI App Initialization
# ============================================================

app = FastAPI(
    title="Sakhi-Score API",
    description="Credit Scoring Engine for Self-Help Groups (SHGs) with Explainable AI",
    version="1.0.0"
)

# CORS Configuration - Allow all origins for hackathon demo
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================
# MongoDB Connection
# ============================================================

MONGO_URI = os.getenv("MONGO_URI", "mongodb://localhost:27017")
DB_NAME = "sakhiscore"
COLLECTION_NAME = "shg_logs"

mongo_client: Optional[MongoClient] = None
db = None
collection = None

def connect_to_mongodb():
    """Initialize MongoDB connection."""
    global mongo_client, db, collection
    try:
        mongo_client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
        # Test connection
        mongo_client.admin.command('ping')
        db = mongo_client[DB_NAME]
        collection = db[COLLECTION_NAME]
        print("тЬЕ Connected to MongoDB successfully!")
        return True
    except ConnectionFailure as e:
        print(f"тЪая╕П MongoDB connection failed: {e}")
        print("ЁЯУЭ Running without database logging...")
        return False

# ============================================================
# Load ML Model
# ============================================================

MODEL_PATH = os.path.join(os.path.dirname(__file__), 'shg_model.pkl')
model = None
explainer = None
whisper_model = None
whisper_backend = None  # 'faster' or 'openai'

def load_model():
    """Load the trained RandomForest model and initialize SHAP explainer."""
    global model, explainer
    
    if not os.path.exists(MODEL_PATH):
        print(f"тЪая╕П Model not found at {MODEL_PATH}")
        print("ЁЯФз Please run 'python model_trainer.py' first!")
        return False
    
    try:
        with open(MODEL_PATH, 'rb') as f:
            model = pickle.load(f)
        
        # Initialize SHAP TreeExplainer for RandomForest
        explainer = shap.TreeExplainer(model)
        print("тЬЕ Model and SHAP explainer loaded successfully!")
        return True
    except Exception as e:
        print(f"тЭМ Error loading model: {e}")
        return False


def load_whisper_model():
    """Load Whisper model for offline speech-to-text with configurable options.

    Prefers `faster-whisper` when available and will attempt to use an
    int8 compute type for lower CPU latency. Configure with env vars:
    - WHISPER_MODEL (default: 'small')
    - WHISPER_DEVICE (default: 'cpu')
    - WHISPER_COMPUTE (optional: e.g. 'int8_float32')
    """
    global whisper_model, whisper_backend
    model_name = os.getenv('WHISPER_MODEL', 'small')
    device = os.getenv('WHISPER_DEVICE', 'cpu')
    compute = os.getenv('WHISPER_COMPUTE', 'int8_float32')

    # Try faster-whisper first (preferred for CPU speedups)
    if _HAS_FASTER_WHISPER:
        try:
            print(f"тЮбя╕П Attempting faster-whisper load: model={model_name}, device={device}, compute={compute}")
            # Some compute types may not be available; try with compute then fallback
            try:
                whisper_model = FasterWhisperModel(model_name, device=device, compute_type=compute)
            except TypeError:
                # Older faster-whisper variations may not accept compute_type
                whisper_model = FasterWhisperModel(model_name, device=device)

            whisper_backend = 'faster'
            print(f"тЬЕ faster-whisper model loaded successfully ({model_name})")
            return True
        except Exception as e:
            print(f"тЪая╕П faster-whisper load failed: {e}")

    # Fallback to OpenAI whisper
    if _HAS_OPENAI_WHISPER:
        try:
            print(f"тЮбя╕П Attempting OpenAI Whisper load: model={model_name}")
            whisper_model = whisper.load_model(model_name)
            whisper_backend = 'openai'
            print(f"тЬЕ OpenAI Whisper model loaded successfully ({model_name})")
            return True
        except Exception as e:
            print(f"тЪая╕П OpenAI Whisper load failed: {e}")

    whisper_model = None
    whisper_backend = None
    return False

# ============================================================
# Pydantic Models
# ============================================================

class PredictionInput(BaseModel):
    """Input schema for credit score prediction."""
    savings: float = Field(..., ge=100, le=5000, description="Savings per member (INR)")
    attendance: float = Field(..., ge=0, le=100, description="Attendance rate (%)")
    repayment: float = Field(..., ge=0, le=100, description="Loan repayment rate (%)")


class TTSRequest(BaseModel):
    """Request model for text-to-speech."""
    text: str
    lang: str = "en"

class PredictionResponse(BaseModel):
    """Response schema for credit score prediction."""
    score: int
    risk: str
    risk_color: str
    explanation_image: str
    features: dict
    timestamp: str

# ============================================================
# Helper Functions
# ============================================================

def calculate_credit_score(savings: float, attendance: float, repayment: float) -> int:
    """
    Calculate credit score based on weighted formula.
    Mirrors the training data generation logic.
    """
    normalized_savings = (savings - 100) / (5000 - 100) * 100
    
    score = (
        0.30 * normalized_savings +
        0.30 * attendance +
        0.40 * repayment
    )
    
    return int(np.clip(score, 0, 100))

def generate_shap_waterfall(input_data: np.ndarray, feature_names: list) -> str:
    """
    Generate SHAP waterfall plot and return as Base64 encoded PNG.
    """
    if explainer is None:
        raise HTTPException(status_code=500, detail="SHAP explainer not initialized")
    
    # Calculate SHAP values
    shap_values = explainer.shap_values(input_data)
    
    # For binary classification, use the positive class (Low Risk)
    if isinstance(shap_values, list):
        shap_vals = shap_values[1][0]  # Class 1 (Low Risk) values
    else:
        shap_vals = shap_values[0]
    
    # Create SHAP Explanation object for waterfall plot
    expected_value = explainer.expected_value
    if isinstance(expected_value, np.ndarray):
        expected_value = expected_value[1]  # Use positive class
    
    explanation = shap.Explanation(
        values=shap_vals,
        base_values=expected_value,
        data=input_data[0],
        feature_names=feature_names
    )
    
    # Create figure with custom styling
    fig, ax = plt.subplots(figsize=(10, 6))
    plt.style.use('default')
    
    # Generate waterfall plot
    shap.plots.waterfall(explanation, max_display=10, show=False)
    
    # Customize the plot
    plt.title("ЁЯФН AI Explanation: What Influenced Your Score?", fontsize=14, fontweight='bold', pad=20)
    plt.xlabel("Impact on Model Output (Low Risk Probability)", fontsize=10)
    plt.tight_layout()
    
    # Save to BytesIO buffer
    buffer = BytesIO()
    plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    plt.close(fig)
    
    # Encode to Base64
    buffer.seek(0)
    img_base64 = base64.b64encode(buffer.read()).decode('utf-8')
    
    return f"data:image/png;base64,{img_base64}"

def log_to_database(input_data: dict, score: int, risk: str) -> bool:
    """Log prediction request to MongoDB."""
    if collection is None:
        return False
    
    try:
        log_entry = {
            "timestamp": datetime.utcnow(),
            "input": input_data,
            "score": score,
            "risk": risk
        }
        collection.insert_one(log_entry)
        return True
    except Exception as e:
        print(f"тЪая╕П Database logging failed: {e}")
        return False

# ============================================================
# API Endpoints
# ============================================================

@app.on_event("startup")
async def startup_event():
    """Initialize connections on app startup."""
    print("\n" + "=" * 50)
    print("ЁЯЪА SAKHI-SCORE API STARTING...")
    print("=" * 50)
    
    connect_to_mongodb()
    load_model()
    load_whisper_model()
    
    print("=" * 50)
    print("тЬЕ API Ready at http://localhost:8000")
    print("ЁЯУЪ Docs at http://localhost:8000/docs")
    print("=" * 50 + "\n")

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on app shutdown."""
    if mongo_client:
        mongo_client.close()
        print("ЁЯСЛ MongoDB connection closed")

@app.get("/")
async def root():
    """Health check endpoint."""
    return {
        "status": "online",
        "app": "Sakhi-Score API",
        "version": "1.0.0",
        "message": "Credit Scoring Engine for Self-Help Groups"
    }

@app.get("/health")
async def health_check():
    """Detailed health check."""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "shap_ready": explainer is not None,
        "database_connected": collection is not None
    }

@app.post("/predict", response_model=PredictionResponse)
async def predict_credit_score(input_data: PredictionInput):
    """
    Predict credit score for a Self-Help Group.
    
    Returns:
    - score: Credit score (0-100)
    - risk: Risk classification (High Risk / Low Risk)
    - explanation_image: SHAP waterfall plot as Base64 PNG
    """
    if model is None:
        raise HTTPException(
            status_code=503, 
            detail="Model not loaded. Please run model_trainer.py first."
        )
    
    # Prepare input for model
    feature_names = ['Savings_Per_Member', 'Attendance_Rate', 'Internal_Loan_Repayment']
    input_array = np.array([[input_data.savings, input_data.attendance, input_data.repayment]])
    
    # Get model prediction (risk classification)
    risk_prediction = model.predict(input_array)[0]
    risk_proba = model.predict_proba(input_array)[0]
    
    # Calculate credit score
    credit_score = calculate_credit_score(
        input_data.savings, 
        input_data.attendance, 
        input_data.repayment
    )
    
    # Determine risk status
    risk_status = "Low Risk тЬЕ" if credit_score >= 60 else "High Risk тЪая╕П"
    risk_color = "#22c55e" if credit_score >= 60 else "#ef4444"
    
    # Generate SHAP explanation image
    try:
        explanation_image = generate_shap_waterfall(input_array, feature_names)
    except Exception as e:
        print(f"тЪая╕П SHAP visualization error: {e}")
        explanation_image = ""
    
    # Log to database
    log_to_database(
        input_data=input_data.dict(),
        score=credit_score,
        risk=risk_status
    )
    
    return PredictionResponse(
        score=credit_score,
        risk=risk_status,
        risk_color=risk_color,
        explanation_image=explanation_image,
        features={
            "savings_per_member": input_data.savings,
            "attendance_rate": input_data.attendance,
            "loan_repayment_rate": input_data.repayment,
            "low_risk_probability": float(risk_proba[1]) if len(risk_proba) > 1 else float(risk_proba[0])
        },
        timestamp=datetime.utcnow().isoformat()
    )

@app.get("/logs")
async def get_prediction_logs(limit: int = 10):
    """Retrieve recent prediction logs from database."""
    if collection is None:
        raise HTTPException(status_code=503, detail="Database not connected")
    
    try:
        logs = list(collection.find({}, {"_id": 0}).sort("timestamp", -1).limit(limit))
        return {"logs": logs, "count": len(logs)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post('/stt')
async def stt_endpoint(file: UploadFile = File(...), lang: str = Form('auto')):
    """Accept an audio upload (multipart/form-data) and return transcription using Whisper.

    - If `lang` is `'auto'` (default), allow the model to auto-detect language.
    - Return `detected_lang` alongside the transcript for frontend UX.
    """
    if whisper_model is None:
        raise HTTPException(status_code=503, detail="Speech model not loaded")

    start = time.time()
    try:
        suffix = Path(file.filename).suffix if file.filename else '.wav'
        with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
            content = await file.read()
            tmp.write(content)
            tmp_path = tmp.name

        print(f"ЁЯФК Received audio file {file.filename} -> {tmp_path} (lang={lang}, size={len(content)})")

        # Decide whether to pass a language hint or let the model auto-detect
        lang_hint = None if (not lang or lang == 'auto') else lang

        # Basic Voice Activity Detection (VAD) filter: drop too-silent uploads
        try:
            import audioop
            rms = audioop.rms(content, 2) if len(content) > 0 else 0
            print(f"ЁЯФН Audio RMS: {rms}")
            if rms < 300:  # threshold tuned for 16-bit audio; may need adjustments
                print("тЪая╕П Audio too silent тАФ skipping STT")
                return {'text': '', 'detected_lang': None, 'duration_ms': int((time.time()-start)*1000), 'note': 'audio too silent'}
        except Exception:
            pass

        text = ''
        detected_lang = None

        # Prefer faster backend when available
        if whisper_backend == 'faster' and whisper_model is not None:
            try:
                segments, info = whisper_model.transcribe(tmp_path, language=lang_hint)
                text = ' '.join([s.text for s in segments]).strip()
                detected_lang = getattr(info, 'language', None)
                print(f"ЁЯУЭ faster-whisper transcription: '{text}' (detected_lang={detected_lang})")
            except Exception as e:
                print(f"тЪая╕П faster-whisper transcription error: {e}. Trying fallback...")
                # fallback attempt with no language hint
                try:
                    segments, info = whisper_model.transcribe(tmp_path, language=None)
                    text = ' '.join([s.text for s in segments]).strip()
                    detected_lang = getattr(info, 'language', None)
                    print(f"ЁЯУЭ faster-whisper fallback transcription: '{text}' (detected_lang={detected_lang})")
                except Exception as e2:
                    print(f"тЭМ faster-whisper fallback failed: {e2}")
                    raise

        elif whisper_backend == 'openai' and whisper_model is not None:
            try:
                result = whisper_model.transcribe(tmp_path, language=lang_hint)
                text = result.get('text', '').strip()
                detected_lang = result.get('language', None)
                print(f"ЁЯУЭ OpenAI Whisper transcription: '{text}' (detected_lang={detected_lang})")
            except Exception as e:
                print(f"тЪая╕П OpenAI Whisper transcription error: {e}")
                raise
        else:
            raise RuntimeError('No speech model available')

        # Basic normalization and cleanup for multilingual robustness
        try:
            text = unicodedata.normalize('NFKC', text).strip()
            text = ' '.join(text.split())
        except Exception:
            pass

        # If text looks empty but we had a lang hint 'auto', try statistical detection
        if (not text or len(text) < 2) and lang in (None, '', 'auto'):
            try:
                # Attempt to guess language from raw audio filename or other heuristics
                guessed = None
                # fallback: try language detect on any existing textual metadata
                guessed = None
                if guessed:
                    detected_lang = guessed
                else:
                    # Try to see if model returned a language attribute earlier
                    detected_lang = detected_lang or None
            except Exception:
                pass

        # Parse spoken numbers into numeric values when set_field intents are used
        # This is left to the `/converse` parsing which can call w2n when needed.

        duration_ms = int((time.time() - start) * 1000)
        return { 'text': text, 'detected_lang': detected_lang, 'duration_ms': duration_ms }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"STT failed: {e}")


@app.post('/converse')
async def converse_endpoint(text: str = Form(...), lang: str = Form('en'),
                            savings: float = Form(None), attendance: float = Form(None), repayment: float = Form(None)):
    """Simple conversational handler: recognizes 'calculate' intent and returns localized reply."""
    t = (text or '').lower()

    # Minimal keywords per language for 'calculate'
    calculate_keywords = {
        'en': ['calculate', 'score', 'compute'],
        'hi': ['рд╕реНрдХреЛрд░', 'рдЧрдгрдирд╛', 'рд╕реНрдХреЛрд░ рдирд┐рдХрд╛рд▓'],
        'bn': ['рж╕рзНржХрзЛрж░', 'рж╣рж┐рж╕рж╛ржм', 'ржЧржгржирж╛'],
        'ta': ['роородро┐рокрпНрокрпАроЯрпБ', 'роХрогроХрпНроХро┐роЯ', 'ро╕рпНроХрпЛро░рпН'],
        'kn': ['р▓╕р│Нр▓Хр│Лр▓░р│Н', 'р▓Чр▓гр▓ир│Ж', 'р▓▓р│Жр▓Хр│Нр▓Х'],
        'te': ['р░╕р▒Нр░Хр▒Лр░░р▒Н', 'р░Чр░гр░и', 'р░▓р▒Жр░Хр▒Нр░Х'],
        'ml': ['р┤╕р╡Нр┤Хр╡Лр╡╝', 'ри▓р▒Жр░Хр▒Нр░Х', 'р┤Чр┤гр┤┐р┤Хр╡Нр┤Х'],
        'mr': ['рд╕реНрдХреЛрд░', 'рдЧрдгрдирд╛', 'рд╣рд┐рд╢реЗрдм'],
        'gu': ['рк╕рлНркХрлЛрк░', 'ркЧркгркдрк░рлА', 'рк╣рк┐рк╕рк╛ркм'],
        'pa': ['ри╕риХрйЛри░', 'риЧригриири╛', 'ри╣ри┐ри╕ри╛рим']
    }

    # Localized reply templates
    TEMPLATES = {
        'en': "Your credit score is {score}. Risk: {risk}.",
        'hi': "рдЖрдкрдХрд╛ рдХреНрд░реЗрдбрд┐рдЯ рд╕реНрдХреЛрд░ {score} рд╣реИред рдЬреЛрдЦрд┐рдо: {risk}ред",
        'bn': "ржЖржкржирж╛рж░ ржХрзНрж░рзЗржбрж┐ржЯ рж╕рзНржХрзЛрж░ {score}ред ржЭрзБржБржХрж┐: {risk}",
        'ta': "роЙроЩрпНроХро│рпН роХроЯройрпН роородро┐рокрпНрокрпЖрогрпН {score}. роЕрокро╛ропроорпН: {risk}",
        'kn': "р▓ир▓┐р▓ор│Нр▓о р▓Хр│Нр▓░р│Жр▓бр▓┐р▓Яр│Н р▓╕р│Нр▓Хр│Лр▓░р│Н {score}. р▓Ер▓кр▓╛р▓п: {risk}",
        'te': "р░ор▒А р░Хр▒Нр░░р▒Жр░бр░┐р░Яр▒Н р░╕р▒Нр░Хр▒Лр░░р▒Б {score}. р░кр▒Нр░░р░ор░╛р░жр░В: {risk}",
        'ml': "р┤ир┤┐р┤Щр╡Нр┤Щр┤│р╡Бр┤Яр╡Ж р┤Хр╡Нр┤░р╡Жр┤бр┤┐р┤▒р╡Нр┤▒р╡Н р┤╕р╡Нр┤Хр╡Лр╡╝ {score}. р┤▒р┤┐р┤╕р╡НтАМр┤Хр╡Н: {risk}",
        'mr': "рдЖрдкрд▓рд╛ рдХреНрд░реЗрдбрд┐рдЯ рд╕реНрдХреЛрдЕрд░ {score}. рдЬреЛрдЦрдо: {risk}",
        'gu': "ркдркорк╛рк░рлЛ ркХрлНрк░рлЗркбрк┐ркЯ рк╕рлНркХрлЛрк░ {score}. ркЬрлЛркЦрко: {risk}",
        'pa': "ридрйБри╣ри╛рибри╛ риХрйНри░рйИрибри┐риЯ ри╕риХрйЛри░ {score}. риЬрйЛриЦрио: {risk}"
    }

    # If caller asked for 'auto' or sent unknown lang, try to detect from text
    if not lang or lang == 'auto' or lang not in calculate_keywords:
        try:
            detected = lang_detect(text or '')
            detected_short = detected[:2]
            if detected_short in calculate_keywords:
                lang = detected_short
                print(f"ЁЯФд converse: auto-detected language -> {lang}")
        except Exception:
            lang = 'en'

    # detect calculate intent
    intent_calculate = any(k in t for k in calculate_keywords.get(lang, []))

    if intent_calculate:
        # Use provided metrics if any, else defaults
        s = float(savings) if savings is not None else 2500.0
        a = float(attendance) if attendance is not None else 85.0
        r = float(repayment) if repayment is not None else 75.0

        score = calculate_credit_score(s, a, r)
        risk_status = "Low Risk" if score >= 60 else "High Risk"

        # Localize risk label
        risk_local = {
            'en': 'Low Risk' if score >= 60 else 'High Risk',
            'hi': 'рдХрдо рдЬреЛрдЦрд┐рдо' if score >= 60 else 'рдЙрдЪреНрдЪ рдЬреЛрдЦрд┐рдо',
            'bn': 'ржХржо ржЭрзБржБржХрж┐' if score >= 60 else 'ржЙржЪрзНржЪ ржЭрзБржБржХрж┐',
            'ta': 'роХрпБро▒рпИроирпНрод роЕрокро╛ропроорпН' if score >= 60 else 'роЙропро░рпНроирпНрод роЕрокро╛ропроорпН',
            'kn': 'р▓Хр▓бр▓┐р▓ор│Ж р▓Ер▓кр▓╛р▓п' if score >= 60 else 'р▓Ер▓зр▓┐р▓Х р▓Ер▓кр▓╛р▓п',
            'te': 'р░др░Хр▒Нр░Хр▒Бр░╡ р░кр▒Нр░░р░ор░╛р░жр░В' if score >= 60 else 'р░Ер░зр░┐р░Х р░кр▒Нр░░р░ор░╛р░жр░В',
            'ml': 'р┤Хр╡Бр┤▒р┤Юр╡Нр┤Ю ржЭр╡В╒й╒л' if score >= 60 else 'р┤Йр┤пр╡╝р┤ир╡Нр┤и р┤▒р┤┐р┤╕р╡НтАМр┤Хр╡Н',
            'mr': 'рдХрдореА рдЬреЛрдЦреАрдо' if score >= 60 else 'рдЬрд╛рд╕реНрдд рдЬреЛрдЦреАрдо',
            'gu': 'ркУркЫрлЛ ркЬрлЛркЦрко' if score >= 60 else 'ркЙркЪрлНркЪ ркЬрлЛркЦрко',
            'pa': 'риШрй▒риЯ риЬрйЛриЦрио' if score >= 60 else 'риЙрй▒риЪ риЬрйЛриЦрио'
        }.get(lang, 'Low Risk' if score >= 60 else 'High Risk')

        template = TEMPLATES.get(lang, TEMPLATES['en'])
        reply = template.format(score=score, risk=risk_local)

        return { 'action': 'predict', 'score': score, 'reply': reply }

    # Fallback: echo
    # Try to detect "set <field> to <value>" commands
    # Field keywords per language
    field_keywords = {
        'en': {
            'savings': ['savings', 'monthly savings', 'monthly saving', 'save'],
            'attendance': ['attendance', 'meeting attendance', 'presence'],
            'repayment': ['repayment', 'repay', 'repayment rate', 'loan repayment']
        },
        'hi': {
            'savings': ['рдмрдЪрдд', 'рдмрдЪрдд рдкреНрд░рддрд┐ рд╕рджрд╕реНрдп', 'рдорд╛рд╕рд┐рдХ рдмрдЪрдд'],
            'attendance': ['рдЙрдкрд╕реНрдерд┐рддрд┐', 'рд╣рд╛рдЬрд┐рд░реА', 'рд╣рд╛рдЬрд┐рд░реА рджрд░'],
            'repayment': ['рднреБрдЧрддрд╛рди', 'рд╡рд╛рдкрд╕реА', 'рдЛрдг рднреБрдЧрддрд╛рди']
        },
        'bn': {
            'savings': ['рж╕ржЮрзНржЪржпрж╝', 'ржорж╛рж╕рж┐ржХ рж╕ржЮрзНржЪржпрж╝'],
            'attendance': ['ржЙржкрж╕рзНржерж┐рждрж┐'],
            'repayment': ['ржкрзНрж░рждрзНржпрж░рзНржкржг', 'ржкрзНрж░рждрж┐ржжрж╛ржи']
        },
        'ta': {
            'savings': ['роЪрпЗрооро┐рокрпНрокрпБ', 'рооро╛род роЪрпЗрооро┐рокрпНрокрпБ'],
            'attendance': ['родро▒рпНрокрпЛродрпИроп', 'ро╡ро░рпБроХрпИ', 'роЙрокрпИропро┐ро▓рпН'],
            'repayment': ['родро┐ро░рпБрокрпНрокрпБ', 'роХроЯрпНроЯрогроорпН']
        },
        'te': {
            'savings': ['р░╕р▒Зр░╡р░┐р░Вр░Чр▒Нр░╕р▒Н', 'р░ор░╛р░╕р░┐р░Х р░╕р▒Зр░╡р░┐р░Вр░Чр▒Нр░╕р▒Н'],
            'attendance': ['р░╣р░╛р░Ьр░░р▒Б', 'р░╣р░╛р░Ьр░░р▒А'],
            'repayment': ['р░Ър▒Жр░▓р▒Нр░▓р░┐р░Вр░кр▒Б', 'р░др░┐р░░р░┐р░Чр░┐ р░Ър▒Жр░▓р▒Нр░▓р░┐р░Вр░кр▒Б']
        },
        'kn': {
            'savings': ['р▓╕р▓Вр▓кр▓╛р▓жр▓ир│Ж', 'р▓ор▓╛р▓╕р▓┐р▓Х р▓Йр▓│р▓╡р│Б'],
            'attendance': ['р▓╣р▓╛р▓Ьр▓░р▓┐', 'р▓╣р▓╛р▓Ьр▓░р▓┐ р▓жр▓░'],
            'repayment': ['родро┐ро░рпБрокрпНрокро┐']
        },
        'ml': {
            'savings': ['р┤╕р╡Зр┤ор┤┐р┤кр╡Нр┤кр╡Н', 'р┤ор┤╛р┤╕р┤┐р┤Хр╡Нр┤╕р╡Зр┤ор┤┐р┤кр╡Нр┤кр╡Н'],
            'attendance': ['р┤╣р┤╛р┤Ьр┤░', 'р┤Йр┤кр┤╕р╡Нр┤ер┤┐р┤др┤┐'],
            'repayment': ['р┤кр┤░р┤┐р┤╢р╡Лр┤зр┤и']
        },
        'mr': {
            'savings': ['рдмрдЪрдд', 'рдорд╛рд╕рд┐рдХ рдмрдЪрдд'],
            'attendance': ['рд╣рдЬреЗрд░реА', 'рдЙрдкрд╕реНрдерд┐рддреА'],
            'repayment': ['рдкрд░рддрдлреЗрдб', 'рд╣рдкреНрддреЗ']
        },
        'gu': {
            'savings': ['ркмркЪркд', 'ркорк╛рк╕рк┐ркХ ркмркЪркд'],
            'attendance': ['рк╣рк╛ркЬрк░рлА', 'рк╣рк╛ркЬрк░рлА ркжрк░'],
            'repayment': ['ркЪрлБркХрк╡ркгрлА', 'рк╡рк╛рккрк╕рлА']
        },
        'pa': {
            'savings': ['римриЪрид', 'риори╣рйАриири╛ри╡ри╛ри░ римриЪрид'],
            'attendance': ['ри╣ри╛риЬри╝ри░рйА', 'ри╣ри╛риЬри╝ри░рйА рижри░'],
            'repayment': ['ри╡ри╛рикри╕рйА', 'риЪрйБриХри╛риВ']
        }
    }

    # Ensure language mapping exists (fallback to en)
    fk = field_keywords.get(lang, field_keywords['en'])

    # Find a number in the text: prefer explicit digits
    import re
    num_match = re.search(r"(\d{1,7}(?:[\.,]\d+)?)", text)
    value = None
    if num_match:
        value_raw = num_match.group(1)
        try:
            value = float(value_raw.replace(',', '')) if ('.' in value_raw or ',' in value_raw) else int(value_raw.replace(',', ''))
        except Exception:
            value = value_raw

    lowered = t
    # If no digits found and language is English, try parsing number words
    if value is None and (lang.startswith('en') or lang == 'en'):
        try:
            # Attempt to extract phrase after known field keyword
            for field, kwlist in fk.items():
                for kw in kwlist:
                    if kw in lowered:
                        cand = lowered.split(kw)[-1]
                        # Keep only words and spaces
                        cand_words = re.sub(r"[^a-zA-Z\s-]", ' ', cand).strip()
                        if cand_words:
                            try:
                                parsed = w2n.word_to_num(cand_words)
                                value = parsed
                                break
                            except Exception:
                                continue
                if value is not None:
                    break
        except Exception:
            value = None

    if value is not None:
        # look for which field
        for field, kwlist in fk.items():
            if any(kw in lowered for kw in kwlist):
                # Compose a localized reply
                replies = {
                    'en': f'Set {field} to {value}.',
                    'hi': f'{field} рдХреЛ {value} рдкрд░ рд╕реЗрдЯ рдХрд░ рджрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИред',
                    'bn': f'{field} {value} ржП рж╕рзЗржЯ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗред'
                }
                reply = replies.get(lang, replies['en'])
                return { 'action': 'set_field', 'field': field, 'value': value, 'reply': reply }

    # Navigation intents per language
    nav_keywords = {
        'en': {
            'dashboard': ['dashboard', 'home'],
            'accounts': ['account', 'accounts'],
            'transactions': ['transaction', 'transactions'],
            'reports': ['report', 'reports'],
            'investments': ['investment', 'investments'],
            'loans': ['loan', 'loans'],
            'settings': ['setting', 'settings'],
            'sakhi_ai': ['sakhi ai', 'sakhi']
        },
        'hi': {
            'dashboard': ['рдбреИрд╢рдмреЛрд░реНрдб', 'рд╣реЛрдо'],
            'accounts': ['рдЦрд╛рддреЗ'],
            'transactions': ['рд▓реЗрди', 'рд▓реЗрди-рджреЗрди'],
            'reports': ['рд░рд┐рдкреЛрд░реНрдЯ'],
            'investments': ['рдирд┐рд╡реЗрд╢'],
            'loans': ['рдЛрдг'],
            'settings': ['рд╕реЗрдЯрд┐рдВрдЧ'],
            'sakhi_ai': ['рд╕рдЦреА']
        }
    }

    # show logs intent keywords
    logs_keywords = {
        'en': ['logs', 'show logs', 'prediction logs', 'history'],
        'hi': ['рд▓реЙрдЧ', 'рд▓реЙрдЧ рджрд┐рдЦрд╛рдПрдБ', 'рдЗрддрд┐рд╣рд╛рд╕']
    }

    # detect navigation
    nk = nav_keywords.get(lang, nav_keywords['en'])
    for target, words in nk.items():
        if any(w in t for w in words):
            replies_nav = {
                'en': f'Navigating to {target}.',
                'hi': f'{target} рдкрд░ рдЬрд╛ рд░рд╣реЗ рд╣реИрдВред'
            }
            return { 'action': 'navigate', 'target': target, 'reply': replies_nav.get(lang, replies_nav['en']) }

    # detect logs request
    lk = logs_keywords.get(lang, logs_keywords['en'])
    if any(w in t for w in lk):
        replies_logs = {
            'en': 'Fetching recent logs for you.',
            'hi': 'рдирд╡реАрдирддрдо рд▓реЙрдЧ рдирд┐рдХрд╛рд▓ рд░рд╣рд╛ рд╣реВрдБред'
        }
        return { 'action': 'show_logs', 'reply': replies_logs.get(lang, replies_logs['en']) }

    return { 'action': 'echo', 'reply': f"I heard: {text}" }


@app.post("/tts")
async def tts_endpoint(request: TTSRequest):
    """Generate speech audio (MP3) for provided text using gTTS."""
    text = (request.text or "").strip()
    if not text:
        raise HTTPException(status_code=400, detail="Text is required for TTS")

    try:
        buffer = BytesIO()
        tts = gTTS(text=text, lang=request.lang)
        tts.write_to_fp(buffer)
        buffer.seek(0)
        return StreamingResponse(buffer, media_type="audio/mpeg")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"TTS generation failed: {e}")

# ============================================================
# Run with Uvicorn
# ============================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
